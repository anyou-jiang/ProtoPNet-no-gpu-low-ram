{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373e7e6c-daea-422d-b595-11caff0d6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "# import torch.utils.data.distributed\n",
    "\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "from helpers import makedir\n",
    "import push\n",
    "import prune\n",
    "import train_and_test as tnt\n",
    "import save\n",
    "from log import create_logger\n",
    "from preprocess import mean, std, preprocess_input_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4bffefa-ef03-4e19-85aa-5fed7358dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting\n",
    "prototype_shape = (128, 8, 1, 1)\n",
    "# num_classes = 200\n",
    "# prototype_shape = (30, 512, 1, 1) # (2000, 128, 1, 1)\n",
    "num_classes = 2\n",
    "prototype_activation_function = 'log'\n",
    "add_on_layers_type = 'regular'\n",
    "\n",
    "experiment_run = '003'\n",
    "\n",
    "\n",
    "\n",
    "joint_optimizer_lrs = {'features': 1e-4,\n",
    "                       'add_on_layers': 3e-3,\n",
    "                       'prototype_vectors': 3e-3}\n",
    "joint_lr_step_size = 5\n",
    "\n",
    "warm_optimizer_lrs = {'add_on_layers': 3e-3,\n",
    "                      'prototype_vectors': 3e-3}\n",
    "\n",
    "last_layer_optimizer_lr = 1e-4\n",
    "\n",
    "coefs = {\n",
    "    'crs_ent': 1,\n",
    "    'clst': 0.8,\n",
    "    'sep': -0.08,\n",
    "    'l1': 1e-4,\n",
    "}\n",
    "\n",
    "num_train_epochs = 1000\n",
    "num_warm_epochs = 5\n",
    "\n",
    "push_start = 10\n",
    "push_epochs = [i for i in range(num_train_epochs) if i % 10 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a539a78-f0b7-4ee9-9e25-e35691112f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_architecture: vgg19\n",
      "base_architecture_type: vgg\n",
      "mode_dir: ./saved_models/vgg19/003/\n",
      "saved main.ipynb to /home/jovyan/codes/ProtoPNet/main.ipynb\n",
      "train dir: ./datasets/cub200_cropped/train_cropped_augmented/\n",
      "test_dir: ./datasets/cub200_cropped/test_cropped/\n",
      "train_push_dir: ./datasets/cub200_cropped/train_cropped/\n"
     ]
    }
   ],
   "source": [
    "## settings\n",
    "base_architecture = 'vgg19'\n",
    "base_architecture_type = re.match('^[a-z]*', base_architecture).group(0)\n",
    "\n",
    "model_dir = './saved_models/' + base_architecture + '/' + experiment_run + '/'\n",
    "makedir(model_dir)\n",
    "\n",
    "my_filepath = os.path.join(os.getcwd(), 'main.ipynb') \n",
    "shutil.copy(src=my_filepath, dst=model_dir)\n",
    "\n",
    "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n",
    "img_dir = os.path.join(model_dir, 'img')\n",
    "makedir(img_dir)\n",
    "weight_matrix_filename = 'outputL_weights'\n",
    "prototype_img_filename_prefix = 'prototype-img'\n",
    "prototype_self_act_filename_prefix = 'prototype-self-act'\n",
    "proto_bound_boxes_filename_prefix = 'bb'\n",
    "\n",
    "log('base_architecture: {0}'.format(base_architecture))\n",
    "log('base_architecture_type: {0}'.format(base_architecture_type))\n",
    "log('mode_dir: {0}'.format(model_dir))\n",
    "log('saved main.ipynb to {0}'.format(my_filepath))\n",
    "\n",
    "data_path = './datasets/cub200_cropped/'\n",
    "train_dir = data_path + 'train_cropped_augmented/'\n",
    "test_dir = data_path + 'test_cropped/'\n",
    "train_push_dir = data_path + 'train_cropped/'\n",
    "\n",
    "log('train dir: {0}'.format(train_dir))\n",
    "log('test_dir: {0}'.format(test_dir))\n",
    "log('train_push_dir: {0}'.format(train_push_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a13ed7-2725-4ea5-a807-5bb4da6ed39a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 60\n",
      "push set size: 60\n",
      "test set size: 60\n",
      "batch size: 5\n"
     ]
    }
   ],
   "source": [
    "## load the dataset\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_size = 56\n",
    "train_batch_size = 5\n",
    "test_batch_size = 5\n",
    "train_push_batch_size = 5\n",
    "\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
    "    num_workers=2, pin_memory=False)\n",
    "# push set\n",
    "train_push_dataset = datasets.ImageFolder(\n",
    "    train_push_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "train_push_loader = torch.utils.data.DataLoader(\n",
    "    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=False)\n",
    "# test set\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    test_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
    "    num_workers=2, pin_memory=False)\n",
    "\n",
    "# we should look into distributed sampler more carefully at torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "log('training set size: {0}'.format(len(train_loader.dataset)))\n",
    "log('push set size: {0}'.format(len(train_push_loader.dataset)))\n",
    "log('test set size: {0}'.format(len(test_loader.dataset)))\n",
    "log('batch size: {0}'.format(train_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6626c6-354c-46c6-b067-df80ed02c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from resnet_features import resnet18_features, resnet34_features, resnet50_features, resnet101_features, resnet152_features\n",
    "from densenet_features import densenet121_features, densenet161_features, densenet169_features, densenet201_features\n",
    "from vgg_features import vgg11_features, vgg11_bn_features, vgg13_features, vgg13_bn_features, vgg16_features, vgg16_bn_features,\\\n",
    "                         vgg19_features, vgg19_bn_features\n",
    "\n",
    "from receptive_field import compute_proto_layer_rf_info_v2\n",
    "\n",
    "base_architecture_to_features = {'resnet18': resnet18_features,\n",
    "                                 'resnet34': resnet34_features,\n",
    "                                 'resnet50': resnet50_features,\n",
    "                                 'resnet101': resnet101_features,\n",
    "                                 'resnet152': resnet152_features,\n",
    "                                 'densenet121': densenet121_features,\n",
    "                                 'densenet161': densenet161_features,\n",
    "                                 'densenet169': densenet169_features,\n",
    "                                 'densenet201': densenet201_features,\n",
    "                                 'vgg11': vgg11_features,\n",
    "                                 'vgg11_bn': vgg11_bn_features,\n",
    "                                 'vgg13': vgg13_features,\n",
    "                                 'vgg13_bn': vgg13_bn_features,\n",
    "                                 'vgg16': vgg16_features,\n",
    "                                 'vgg16_bn': vgg16_bn_features,\n",
    "                                 'vgg19': vgg19_features,\n",
    "                                 'vgg19_bn': vgg19_bn_features}\n",
    "\n",
    "class PPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, features, img_size, prototype_shape,\n",
    "                 proto_layer_rf_info, num_classes,\n",
    "                 prototype_activation_function='log',\n",
    "                 add_on_layers_type='bottleneck'):\n",
    "\n",
    "        super(PPNet, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.prototype_shape = prototype_shape\n",
    "        self.num_prototypes = prototype_shape[0]\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = 1e-4\n",
    "        \n",
    "        # prototype_activation_function could be 'log', 'linear',\n",
    "        # or a generic function that converts distance to similarity score\n",
    "        self.prototype_activation_function = prototype_activation_function\n",
    "\n",
    "        '''\n",
    "        Here we are initializing the class identities of the prototypes\n",
    "        Without domain specific knowledge we allocate the same number of\n",
    "        prototypes for each class\n",
    "        '''\n",
    "        assert(self.num_prototypes % self.num_classes == 0)\n",
    "        # a onehot indication matrix for each prototype's class identity\n",
    "        self.prototype_class_identity = torch.zeros(self.num_prototypes,\n",
    "                                                    self.num_classes)\n",
    "\n",
    "        num_prototypes_per_class = self.num_prototypes // self.num_classes\n",
    "        for j in range(self.num_prototypes):\n",
    "            self.prototype_class_identity[j, j // num_prototypes_per_class] = 1\n",
    "\n",
    "        self.proto_layer_rf_info = proto_layer_rf_info\n",
    "\n",
    "        # this has to be named features to allow the precise loading\n",
    "        self.features = features\n",
    "        first_add_on_layer_in_channels = [i for i in features.modules() if isinstance(i, nn.Conv2d)][-1].out_channels\n",
    "\n",
    "        self.add_on_layers = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=first_add_on_layer_in_channels, out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=self.prototype_shape[1], out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "        \n",
    "        self.prototype_vectors = nn.Parameter(torch.rand(self.prototype_shape),\n",
    "                                              requires_grad=True)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.prototype_shape),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.last_layer = nn.Linear(self.num_prototypes, self.num_classes,\n",
    "                                    bias=False) # do not use bias\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def conv_features(self, x):\n",
    "        '''\n",
    "        the feature input to prototype layer\n",
    "        '''\n",
    "        x = self.features(x)\n",
    "        x = self.add_on_layers(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _weighted_l2_convolution(input, filter, weights):\n",
    "        '''\n",
    "        input of shape N * c * h * w\n",
    "        filter of shape P * c * h1 * w1\n",
    "        weight of shape P * c * h1 * w1\n",
    "        '''\n",
    "        input2 = input ** 2\n",
    "        input_patch_weighted_norm2 = F.conv2d(input=input2, weight=weights)\n",
    "\n",
    "        filter2 = filter ** 2\n",
    "        weighted_filter2 = filter2 * weights\n",
    "        filter_weighted_norm2 = torch.sum(weighted_filter2, dim=(1, 2, 3))\n",
    "        filter_weighted_norm2_reshape = filter_weighted_norm2.view(-1, 1, 1)\n",
    "\n",
    "        weighted_filter = filter * weights\n",
    "        weighted_inner_product = F.conv2d(input=input, weight=weighted_filter)\n",
    "\n",
    "        # use broadcast\n",
    "        intermediate_result = \\\n",
    "            - 2 * weighted_inner_product + filter_weighted_norm2_reshape\n",
    "        # x2_patch_sum and intermediate_result are of the same shape\n",
    "        distances = F.relu(input_patch_weighted_norm2 + intermediate_result)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _l2_convolution(self, x):\n",
    "        '''\n",
    "        apply self.prototype_vectors as l2-convolution filters on input x\n",
    "        '''\n",
    "        x2 = x ** 2\n",
    "        x2_patch_sum = F.conv2d(input=x2, weight=self.ones)\n",
    "\n",
    "        p2 = self.prototype_vectors ** 2\n",
    "        p2 = torch.sum(p2, dim=(1, 2, 3))\n",
    "        # p2 is a vector of shape (num_prototypes,)\n",
    "        # then we reshape it to (num_prototypes, 1, 1)\n",
    "        p2_reshape = p2.view(-1, 1, 1)\n",
    "\n",
    "        xp = F.conv2d(input=x, weight=self.prototype_vectors)\n",
    "        intermediate_result = - 2 * xp + p2_reshape  # use broadcast\n",
    "        # x2_patch_sum and intermediate_result are of the same shape\n",
    "        distances = F.relu(x2_patch_sum + intermediate_result)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def prototype_distances(self, x):\n",
    "        '''\n",
    "        x is the raw input\n",
    "        '''\n",
    "        conv_features = self.conv_features(x)\n",
    "        distances = self._l2_convolution(conv_features)\n",
    "        return distances\n",
    "\n",
    "    def distance_2_similarity(self, distances):\n",
    "        if self.prototype_activation_function == 'log':\n",
    "            return torch.log((distances + 1) / (distances + self.epsilon))\n",
    "        elif self.prototype_activation_function == 'linear':\n",
    "            return -distances\n",
    "        else:\n",
    "            return self.prototype_activation_function(distances)\n",
    "\n",
    "    def forward(self, x):\n",
    "        distances = self.prototype_distances(x)\n",
    "        '''\n",
    "        we cannot refactor the lines below for similarity scores\n",
    "        because we need to return min_distances\n",
    "        '''\n",
    "        # global min pooling\n",
    "        min_distances = -F.max_pool2d(-distances,\n",
    "                                      kernel_size=(distances.size()[2],\n",
    "                                                   distances.size()[3]))\n",
    "        min_distances = min_distances.view(-1, self.num_prototypes)\n",
    "        prototype_activations = self.distance_2_similarity(min_distances)\n",
    "        logits = self.last_layer(prototype_activations)\n",
    "        return logits, min_distances\n",
    "\n",
    "    def push_forward(self, x):\n",
    "        '''this method is needed for the pushing operation'''\n",
    "        conv_output = self.conv_features(x)\n",
    "        distances = self._l2_convolution(conv_output)\n",
    "        return conv_output, distances\n",
    "\n",
    "    def prune_prototypes(self, prototypes_to_prune):\n",
    "        '''\n",
    "        prototypes_to_prune: a list of indices each in\n",
    "        [0, current number of prototypes - 1] that indicates the prototypes to\n",
    "        be removed\n",
    "        '''\n",
    "        prototypes_to_keep = list(set(range(self.num_prototypes)) - set(prototypes_to_prune))\n",
    "\n",
    "        self.prototype_vectors = nn.Parameter(self.prototype_vectors.data[prototypes_to_keep, ...],\n",
    "                                              requires_grad=True)\n",
    "\n",
    "        self.prototype_shape = list(self.prototype_vectors.size())\n",
    "        self.num_prototypes = self.prototype_shape[0]\n",
    "\n",
    "        # changing self.last_layer in place\n",
    "        # changing in_features and out_features make sure the numbers are consistent\n",
    "        self.last_layer.in_features = self.num_prototypes\n",
    "        self.last_layer.out_features = self.num_classes\n",
    "        self.last_layer.weight.data = self.last_layer.weight.data[:, prototypes_to_keep]\n",
    "\n",
    "        # self.ones is nn.Parameter\n",
    "        self.ones = nn.Parameter(self.ones.data[prototypes_to_keep, ...],\n",
    "                                 requires_grad=False)\n",
    "        # self.prototype_class_identity is torch tensor\n",
    "        # so it does not need .data access for value update\n",
    "        self.prototype_class_identity = self.prototype_class_identity[prototypes_to_keep, :]\n",
    "\n",
    "    def __repr__(self):\n",
    "        # PPNet(self, features, img_size, prototype_shape,\n",
    "        # proto_layer_rf_info, num_classes, init_weights=True):\n",
    "        rep = (\n",
    "            'PPNet(\\n'\n",
    "            '\\tfeatures: {},\\n'\n",
    "            '\\timg_size: {},\\n'\n",
    "            '\\tprototype_shape: {},\\n'\n",
    "            '\\tproto_layer_rf_info: {},\\n'\n",
    "            '\\tnum_classes: {},\\n'\n",
    "            '\\tepsilon: {}\\n'\n",
    "            ')'\n",
    "        )\n",
    "\n",
    "        return rep.format(self.features,\n",
    "                          self.img_size,\n",
    "                          self.prototype_shape,\n",
    "                          self.proto_layer_rf_info,\n",
    "                          self.num_classes,\n",
    "                          self.epsilon)\n",
    "\n",
    "    def set_last_layer_incorrect_connection(self, incorrect_strength):\n",
    "        '''\n",
    "        the incorrect strength will be actual strength if -0.5 then input -0.5\n",
    "        '''\n",
    "        positive_one_weights_locations = torch.t(self.prototype_class_identity)\n",
    "        negative_one_weights_locations = 1 - positive_one_weights_locations\n",
    "\n",
    "        correct_class_connection = 1\n",
    "        incorrect_class_connection = incorrect_strength\n",
    "        self.last_layer.weight.data.copy_(\n",
    "            correct_class_connection * positive_one_weights_locations\n",
    "            + incorrect_class_connection * negative_one_weights_locations)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.add_on_layers.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # every init technique has an underscore _ in the name\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.set_last_layer_incorrect_connection(incorrect_strength=-0.5)\n",
    "\n",
    "\n",
    "\n",
    "def construct_PPNet(base_architecture, pretrained=True, img_size=224,\n",
    "                    prototype_shape=(2000, 512, 1, 1), num_classes=200,\n",
    "                    prototype_activation_function='log',\n",
    "                    add_on_layers_type='bottleneck'):\n",
    "    features = vgg19_features(pretrained=pretrained)\n",
    "    layer_filter_sizes, layer_strides, layer_paddings = features.conv_info()\n",
    "    proto_layer_rf_info = compute_proto_layer_rf_info_v2(img_size=img_size,\n",
    "                                                         layer_filter_sizes=layer_filter_sizes,\n",
    "                                                         layer_strides=layer_strides,\n",
    "                                                         layer_paddings=layer_paddings,\n",
    "                                                         prototype_kernel_size=prototype_shape[2])\n",
    "    return PPNet(features=features,\n",
    "                 img_size=img_size,\n",
    "                 prototype_shape=prototype_shape,\n",
    "                 proto_layer_rf_info=proto_layer_rf_info,\n",
    "                 num_classes=num_classes,\n",
    "                 prototype_activation_function=prototype_activation_function,\n",
    "                 add_on_layers_type=add_on_layers_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50439f05-39ae-4f10-9c87-6a8b251253a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct the model\n",
    "ppnet = construct_PPNet(base_architecture=base_architecture,\n",
    "                              pretrained=True, img_size=img_size,\n",
    "                              prototype_shape=prototype_shape,\n",
    "                              num_classes=num_classes,\n",
    "                              prototype_activation_function=prototype_activation_function,\n",
    "                              add_on_layers_type=add_on_layers_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a224a04b-b253-4223-b6a0-944c9236d121",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppnet_multi = torch.nn.DataParallel(ppnet)\n",
    "class_specific = True\n",
    "\n",
    "# define optimizer\n",
    "joint_optimizer_specs = \\\n",
    "[{'params': ppnet.features.parameters(), 'lr': joint_optimizer_lrs['features'], 'weight_decay': 1e-3}, # bias are now also being regularized\n",
    " {'params': ppnet.add_on_layers.parameters(), 'lr': joint_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet.prototype_vectors, 'lr': joint_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.1)\n",
    "\n",
    "warm_optimizer_specs = \\\n",
    "[{'params': ppnet.add_on_layers.parameters(), 'lr': warm_optimizer_lrs['add_on_layers'], 'weight_decay': 1e-3},\n",
    " {'params': ppnet.prototype_vectors, 'lr': warm_optimizer_lrs['prototype_vectors']},\n",
    "]\n",
    "warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n",
    "\n",
    "last_layer_optimizer_specs = [{'params': ppnet.last_layer.parameters(), 'lr': last_layer_optimizer_lr}]\n",
    "last_layer_optimizer = torch.optim.Adam(last_layer_optimizer_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8e6a-9bfc-4b90-b1df-dc003711f0ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "epoch: \t0\n",
      "\twarm\n",
      "\ttrain\n",
      "\t after model train()\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "log('start training')\n",
    "import copy\n",
    "for epoch in range(num_train_epochs):\n",
    "    log('epoch: \\t{0}'.format(epoch))\n",
    "\n",
    "    if epoch < num_warm_epochs:\n",
    "        tnt.warm_only(model=ppnet_multi, log=log)\n",
    "        _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=warm_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "    else:\n",
    "        tnt.joint(model=ppnet_multi, log=log)\n",
    "        joint_lr_scheduler.step()\n",
    "        _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=joint_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "\n",
    "    accu = tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                    class_specific=class_specific, log=log)\n",
    "    save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'nopush', accu=accu,\n",
    "                                target_accu=0.70, log=log)\n",
    "\n",
    "    if epoch >= push_start and epoch in push_epochs:\n",
    "        push.push_prototypes(\n",
    "            train_push_loader, # pytorch dataloader (must be unnormalized in [0,1])\n",
    "            prototype_network_parallel=ppnet_multi, # pytorch network with prototype_vectors\n",
    "            class_specific=class_specific,\n",
    "            preprocess_input_function=preprocess_input_function, # normalize if needed\n",
    "            prototype_layer_stride=1,\n",
    "            root_dir_for_saving_prototypes=img_dir, # if not None, prototypes will be saved here\n",
    "            epoch_number=epoch, # if not provided, prototypes saved previously will be overwritten\n",
    "            prototype_img_filename_prefix=prototype_img_filename_prefix,\n",
    "            prototype_self_act_filename_prefix=prototype_self_act_filename_prefix,\n",
    "            proto_bound_boxes_filename_prefix=proto_bound_boxes_filename_prefix,\n",
    "            save_prototype_class_identity=True,\n",
    "            log=log)\n",
    "        accu = tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                        class_specific=class_specific, log=log)\n",
    "        save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + 'push', accu=accu,\n",
    "                                    target_accu=0.70, log=log)\n",
    "\n",
    "        if prototype_activation_function != 'linear':\n",
    "            tnt.last_only(model=ppnet_multi, log=log)\n",
    "            for i in range(20):\n",
    "                log('iteration: \\t{0}'.format(i))\n",
    "                _ = tnt.train(model=ppnet_multi, dataloader=train_loader, optimizer=last_layer_optimizer,\n",
    "                              class_specific=class_specific, coefs=coefs, log=log)\n",
    "                accu = tnt.test(model=ppnet_multi, dataloader=test_loader,\n",
    "                                class_specific=class_specific, log=log)\n",
    "                save.save_model_w_condition(model=ppnet, model_dir=model_dir, model_name=str(epoch) + '_' + str(i) + 'push', accu=accu,\n",
    "                                            target_accu=0.70, log=log)\n",
    "   \n",
    "logclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee371aa-440c-4116-ab0c-6e486d518a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
