{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e7e6c-daea-422d-b595-11caff0d6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import argparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08e071-09ae-4003-aac2-d956eaeb43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utility functions\n",
    "def makedir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def create_logger(log_filename, display=True):\n",
    "    f = open(log_filename, 'a')\n",
    "    counter = [0]\n",
    "    # this function will still have access to f after create_logger terminates\n",
    "    def logger(text):\n",
    "        if display:\n",
    "            print(text)\n",
    "        f.write(text + '\\n')\n",
    "        counter[0] += 1\n",
    "        if counter[0] % 10 == 0:\n",
    "            f.flush()\n",
    "            os.fsync(f.fileno())\n",
    "        # Question: do we need to flush()\n",
    "    return logger, f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bffefa-ef03-4e19-85aa-5fed7358dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting\n",
    "prototype_shape = (128, 8, 1, 1)\n",
    "num_classes = 2\n",
    "prototype_activation_function = 'log'\n",
    "add_on_layers_type = 'regular'\n",
    "\n",
    "experiment_run = '003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a539a78-f0b7-4ee9-9e25-e35691112f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## settings\n",
    "base_architecture = 'vgg19'\n",
    "base_architecture_type = re.match('^[a-z]*', base_architecture).group(0)\n",
    "\n",
    "model_dir = './saved_models/' + base_architecture + '/' + experiment_run + '/'\n",
    "makedir(model_dir)\n",
    "\n",
    "my_filepath = os.path.join(os.getcwd(), 'main.ipynb') \n",
    "shutil.copy(src=my_filepath, dst=model_dir)\n",
    "\n",
    "log, logclose = create_logger(log_filename=os.path.join(model_dir, 'train.log'))\n",
    "img_dir = os.path.join(model_dir, 'img')\n",
    "makedir(img_dir)\n",
    "weight_matrix_filename = 'outputL_weights'\n",
    "prototype_img_filename_prefix = 'prototype-img'\n",
    "prototype_self_act_filename_prefix = 'prototype-self-act'\n",
    "proto_bound_boxes_filename_prefix = 'bb'\n",
    "\n",
    "log('base_architecture: {0}'.format(base_architecture))\n",
    "log('base_architecture_type: {0}'.format(base_architecture_type))\n",
    "log('mode_dir: {0}'.format(model_dir))\n",
    "log('saved main.ipynb to {0}'.format(my_filepath))\n",
    "\n",
    "data_path = './datasets/cub200_cropped/'\n",
    "train_dir = data_path + 'train_cropped_augmented/'\n",
    "test_dir = data_path + 'test_cropped/'\n",
    "train_push_dir = data_path + 'train_cropped/'\n",
    "\n",
    "log('train dir: {0}'.format(train_dir))\n",
    "log('test_dir: {0}'.format(test_dir))\n",
    "log('train_push_dir: {0}'.format(train_push_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a13ed7-2725-4ea5-a807-5bb4da6ed39a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load the dataset\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_size = 56\n",
    "train_batch_size = 5\n",
    "test_batch_size = 5\n",
    "train_push_batch_size = 5\n",
    "\n",
    "preprocess_mean = (0.485, 0.456, 0.406)\n",
    "preprocess_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "normalize = transforms.Normalize(mean=preprocess_mean, std=preprocess_std)\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=False)\n",
    "# push set\n",
    "train_push_dataset = datasets.ImageFolder(\n",
    "    train_push_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "train_push_loader = torch.utils.data.DataLoader(\n",
    "    train_push_dataset, batch_size=train_push_batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=False)\n",
    "# test set\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    test_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=False)\n",
    "\n",
    "log('INFO: training set size: {0}'.format(len(train_loader.dataset)))\n",
    "log('INFO: push set size: {0}'.format(len(train_push_loader.dataset)))\n",
    "log('INFO: test set size: {0}'.format(len(test_loader.dataset)))\n",
    "log('INFO: batch size: {0}'.format(train_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f91f4-fc3c-484c-8195-0424d0e6f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vgg_features\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "class VGG_features(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(VGG_features, self).__init__()\n",
    "        self.kernel_sizes = []\n",
    "        self.strides = []\n",
    "        self.paddings = []\n",
    "        self.features = self._make_layers(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        self.n_layers = 0\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "                self.kernel_sizes.append(2)\n",
    "                self.strides.append(2)\n",
    "                self.paddings.append(0)\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                self.n_layers += 1\n",
    "                self.kernel_sizes.append(3)\n",
    "                self.strides.append(1)\n",
    "                self.paddings.append(1)\n",
    "                in_channels = v\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def conv_info(self):\n",
    "        return self.kernel_sizes, self.strides, self.paddings\n",
    "\n",
    "    def num_layers(self):\n",
    "        '''\n",
    "        the number of conv layers in the network\n",
    "        '''\n",
    "        return self.n_layers\n",
    "\n",
    "    def __repr__(self):\n",
    "        template = 'VGG{}'\n",
    "        return template.format(self.num_layers() + 3)\n",
    "\n",
    "def vgg19_features():\n",
    "    \"\"\"VGG 19-layer model\n",
    "    \"\"\"\n",
    "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "    model = VGG_features(cfg)\n",
    "    my_dict = model_zoo.load_url('https://download.pytorch.org/models/vgg19-dcbb9e9d.pth', model_dir='./pretrained_models')\n",
    "    keys_to_remove = set()\n",
    "    \n",
    "    for key in my_dict:\n",
    "        if key.startswith('classifier'):\n",
    "            keys_to_remove.add(key)\n",
    "    for key in keys_to_remove:\n",
    "        del my_dict[key]\n",
    "    model.load_state_dict(my_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3fb1b-132d-41d5-b0bd-22b3adcd5846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_proto_layer_rf_info_v2(img_size, layer_filter_sizes, layer_strides, layer_paddings, prototype_kernel_size):\n",
    "\n",
    "    assert(len(layer_filter_sizes) == len(layer_strides))\n",
    "    assert(len(layer_filter_sizes) == len(layer_paddings))\n",
    "\n",
    "    rf_info = [img_size, 1, 1, 0.5]\n",
    "\n",
    "    for i in range(len(layer_filter_sizes)):\n",
    "        filter_size = layer_filter_sizes[i]\n",
    "        stride_size = layer_strides[i]\n",
    "        padding_size = layer_paddings[i]\n",
    "\n",
    "        rf_info = compute_layer_rf_info(layer_filter_size=filter_size,\n",
    "                                layer_stride=stride_size,\n",
    "                                layer_padding=padding_size,\n",
    "                                previous_layer_rf_info=rf_info)\n",
    "\n",
    "    proto_layer_rf_info = compute_layer_rf_info(layer_filter_size=prototype_kernel_size,\n",
    "                                                layer_stride=1,\n",
    "                                                layer_padding='VALID',\n",
    "                                                previous_layer_rf_info=rf_info)\n",
    "\n",
    "    return proto_layer_rf_info\n",
    "\n",
    "def compute_layer_rf_info(layer_filter_size, layer_stride, layer_padding,\n",
    "                          previous_layer_rf_info):\n",
    "    n_in = previous_layer_rf_info[0] # input size\n",
    "    j_in = previous_layer_rf_info[1] # receptive field jump of input layer\n",
    "    r_in = previous_layer_rf_info[2] # receptive field size of input layer\n",
    "    start_in = previous_layer_rf_info[3] # center of receptive field of input layer\n",
    "\n",
    "    if layer_padding == 'SAME':\n",
    "        n_out = math.ceil(float(n_in) / float(layer_stride))\n",
    "        if (n_in % layer_stride == 0):\n",
    "            pad = max(layer_filter_size - layer_stride, 0)\n",
    "        else:\n",
    "            pad = max(layer_filter_size - (n_in % layer_stride), 0)\n",
    "        assert(n_out == math.floor((n_in - layer_filter_size + pad)/layer_stride) + 1) # sanity check\n",
    "        assert(pad == (n_out-1)*layer_stride - n_in + layer_filter_size) # sanity check\n",
    "    elif layer_padding == 'VALID':\n",
    "        n_out = math.ceil(float(n_in - layer_filter_size + 1) / float(layer_stride))\n",
    "        pad = 0\n",
    "        assert(n_out == math.floor((n_in - layer_filter_size + pad)/layer_stride) + 1) # sanity check\n",
    "        assert(pad == (n_out-1)*layer_stride - n_in + layer_filter_size) # sanity check\n",
    "    else:\n",
    "        # layer_padding is an int that is the amount of padding on one side\n",
    "        pad = layer_padding * 2\n",
    "        n_out = math.floor((n_in - layer_filter_size + pad)/layer_stride) + 1\n",
    "\n",
    "    pL = math.floor(pad/2)\n",
    "\n",
    "    j_out = j_in * layer_stride\n",
    "    r_out = r_in + (layer_filter_size - 1)*j_in\n",
    "    start_out = start_in + ((layer_filter_size - 1)/2 - pL)*j_in\n",
    "    return [n_out, j_out, r_out, start_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6626c6-354c-46c6-b067-df80ed02c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.nn.functional as F\n",
    "# from receptive_field import compute_proto_layer_rf_info_v2\n",
    "\n",
    "class PPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, features, img_size, prototype_shape,\n",
    "                 proto_layer_rf_info, num_classes,\n",
    "                 prototype_activation_function='log',\n",
    "                 add_on_layers_type='bottleneck'):\n",
    "\n",
    "        super(PPNet, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.prototype_shape = prototype_shape\n",
    "        self.num_prototypes = prototype_shape[0]\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = 1e-4\n",
    "        \n",
    "        # prototype_activation_function could be 'log', 'linear',\n",
    "        # or a generic function that converts distance to similarity score\n",
    "        self.prototype_activation_function = prototype_activation_function\n",
    "\n",
    "        '''\n",
    "        Here we are initializing the class identities of the prototypes\n",
    "        Without domain specific knowledge we allocate the same number of\n",
    "        prototypes for each class\n",
    "        '''\n",
    "        assert(self.num_prototypes % self.num_classes == 0)\n",
    "        # a onehot indication matrix for each prototype's class identity\n",
    "        self.prototype_class_identity = torch.zeros(self.num_prototypes,\n",
    "                                                    self.num_classes)\n",
    "\n",
    "        num_prototypes_per_class = self.num_prototypes // self.num_classes\n",
    "        for j in range(self.num_prototypes):\n",
    "            self.prototype_class_identity[j, j // num_prototypes_per_class] = 1\n",
    "\n",
    "        self.proto_layer_rf_info = proto_layer_rf_info\n",
    "\n",
    "        # this has to be named features to allow the precise loading\n",
    "        self.features = features\n",
    "        first_add_on_layer_in_channels = [i for i in features.modules() if isinstance(i, nn.Conv2d)][-1].out_channels\n",
    "\n",
    "        self.add_on_layers = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=first_add_on_layer_in_channels, out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=self.prototype_shape[1], out_channels=self.prototype_shape[1], kernel_size=1),\n",
    "                nn.Sigmoid()\n",
    "                )\n",
    "        \n",
    "        self.prototype_vectors = nn.Parameter(torch.rand(self.prototype_shape),\n",
    "                                              requires_grad=True)\n",
    "\n",
    "        self.ones = nn.Parameter(torch.ones(self.prototype_shape),\n",
    "                                 requires_grad=False)\n",
    "\n",
    "        self.h_last_layer = nn.Linear(self.num_prototypes, self.num_classes,\n",
    "                                    bias=False) # do not use bias\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_out = self.f_conv_layer(x)\n",
    "        g_p_out, min_distances, _ = self.g_p_layer(f_out)\n",
    "        h_out = self.h_last_layer(g_p_out)\n",
    "        return h_out, min_distances    \n",
    "\n",
    "    def f_conv_layer(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.add_on_layers(x)\n",
    "        return x\n",
    "\n",
    "    def g_p_layer(self, z):\n",
    "        z2 = z ** 2\n",
    "        z2_patch_sum = F.conv2d(input=z2, weight=self.ones)\n",
    "\n",
    "        p2 = self.prototype_vectors ** 2\n",
    "        p2 = torch.sum(p2, dim=(1, 2, 3))\n",
    "        p2_reshape = p2.view(-1, 1, 1)\n",
    "\n",
    "        zp = F.conv2d(input=z, weight=self.prototype_vectors)\n",
    "        intermediate_result = - 2 * zp + p2_reshape\n",
    "        zp_distances = F.relu(z2_patch_sum + intermediate_result)\n",
    "\n",
    "        min_zp_distance = -F.max_pool2d(-zp_distances,\n",
    "                                      kernel_size=(zp_distances.size()[2],\n",
    "                                                   zp_distances.size()[3]))\n",
    "        min_zp_distance = min_zp_distance.view(-1, self.num_prototypes)\n",
    "        g_p_out = self.distance_2_similarity(min_zp_distance)        \n",
    "\n",
    "        return g_p_out, min_zp_distance, zp_distances\n",
    "\n",
    "    def distance_2_similarity(self, distances):\n",
    "        return torch.log((distances + 1) / (distances + self.epsilon))\n",
    "\n",
    "    def push_forward(self, x):\n",
    "        '''this method is needed for the pushing operation'''\n",
    "        f_out = self.f_conv_layer(x)\n",
    "        g_p_out, min_distances, distances = self.g_p_layer(f_out)\n",
    "        #return g_p_out, distances\n",
    "        return f_out, distances\n",
    "\n",
    "    def prune_prototypes(self, prototypes_to_prune):\n",
    "        '''\n",
    "        prototypes_to_prune: a list of indices each in\n",
    "        [0, current number of prototypes - 1] that indicates the prototypes to\n",
    "        be removed\n",
    "        '''\n",
    "        prototypes_to_keep = list(set(range(self.num_prototypes)) - set(prototypes_to_prune))\n",
    "\n",
    "        self.prototype_vectors = nn.Parameter(self.prototype_vectors.data[prototypes_to_keep, ...],\n",
    "                                              requires_grad=True)\n",
    "\n",
    "        self.prototype_shape = list(self.prototype_vectors.size())\n",
    "        self.num_prototypes = self.prototype_shape[0]\n",
    "\n",
    "        # changing self.h_last_layer in place\n",
    "        # changing in_features and out_features make sure the numbers are consistent\n",
    "        self.h_last_layer.in_features = self.num_prototypes\n",
    "        self.h_last_layer.out_features = self.num_classes\n",
    "        self.h_last_layer.weight.data = self.h_last_layer.weight.data[:, prototypes_to_keep]\n",
    "\n",
    "        # self.ones is nn.Parameter\n",
    "        self.ones = nn.Parameter(self.ones.data[prototypes_to_keep, ...],\n",
    "                                 requires_grad=False)\n",
    "        # self.prototype_class_identity is torch tensor\n",
    "        # so it does not need .data access for value update\n",
    "        self.prototype_class_identity = self.prototype_class_identity[prototypes_to_keep, :]\n",
    "\n",
    "    def __repr__(self):\n",
    "        # PPNet(self, features, img_size, prototype_shape,\n",
    "        # proto_layer_rf_info, num_classes, init_weights=True):\n",
    "        rep = (\n",
    "            'PPNet(\\n'\n",
    "            '\\tfeatures: {},\\n'\n",
    "            '\\timg_size: {},\\n'\n",
    "            '\\tprototype_shape: {},\\n'\n",
    "            '\\tproto_layer_rf_info: {},\\n'\n",
    "            '\\tnum_classes: {},\\n'\n",
    "            '\\tepsilon: {}\\n'\n",
    "            ')'\n",
    "        )\n",
    "\n",
    "        return rep.format(self.features,\n",
    "                          self.img_size,\n",
    "                          self.prototype_shape,\n",
    "                          self.proto_layer_rf_info,\n",
    "                          self.num_classes,\n",
    "                          self.epsilon)\n",
    "\n",
    "    def set_h_last_layer_incorrect_connection(self, incorrect_strength):\n",
    "        '''\n",
    "        the incorrect strength will be actual strength if -0.5 then input -0.5\n",
    "        '''\n",
    "        positive_one_weights_locations = torch.t(self.prototype_class_identity)\n",
    "        negative_one_weights_locations = 1 - positive_one_weights_locations\n",
    "\n",
    "        correct_class_connection = 1\n",
    "        incorrect_class_connection = incorrect_strength\n",
    "        self.h_last_layer.weight.data.copy_(\n",
    "            correct_class_connection * positive_one_weights_locations\n",
    "            + incorrect_class_connection * negative_one_weights_locations)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.add_on_layers.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # every init technique has an underscore _ in the name\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.set_h_last_layer_incorrect_connection(incorrect_strength=-0.5)\n",
    "\n",
    "\n",
    "\n",
    "def construct_PPNet(base_architecture, pretrained, img_size,\n",
    "                    prototype_shape, num_classes,\n",
    "                    prototype_activation_function='log',\n",
    "                    add_on_layers_type='bottleneck'):\n",
    "    features = vgg19_features()\n",
    "    layer_filter_sizes, layer_strides, layer_paddings = features.conv_info()\n",
    "    proto_layer_rf_info = compute_proto_layer_rf_info_v2(img_size=img_size,\n",
    "                                                         layer_filter_sizes=layer_filter_sizes,\n",
    "                                                         layer_strides=layer_strides,\n",
    "                                                         layer_paddings=layer_paddings,\n",
    "                                                         prototype_kernel_size=prototype_shape[2])\n",
    "    return PPNet(features=features,\n",
    "                 img_size=img_size,\n",
    "                 prototype_shape=prototype_shape,\n",
    "                 proto_layer_rf_info=proto_layer_rf_info,\n",
    "                 num_classes=num_classes,\n",
    "                 prototype_activation_function=prototype_activation_function,\n",
    "                 add_on_layers_type=add_on_layers_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50439f05-39ae-4f10-9c87-6a8b251253a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct the model\n",
    "ppnet = construct_PPNet(base_architecture=base_architecture,\n",
    "                              pretrained=True, img_size=img_size,\n",
    "                              prototype_shape=prototype_shape,\n",
    "                              num_classes=num_classes,\n",
    "                              prototype_activation_function=prototype_activation_function,\n",
    "                              add_on_layers_type=add_on_layers_type)\n",
    "\n",
    "warm_optimizer_lrs = {'add_on_layers': 3e-3, \n",
    "                      'prototype_vectors': 3e-3}\n",
    "\n",
    "warm_optimizer_specs = [{\n",
    "    'params': ppnet.add_on_layers.parameters(), \n",
    "    'lr': warm_optimizer_lrs['add_on_layers'], \n",
    "    'weight_decay': 1e-3},                     \n",
    "    {'params': ppnet.prototype_vectors, \n",
    "    'lr': warm_optimizer_lrs['prototype_vectors']\n",
    "    }]\n",
    "\n",
    "warm_optimizer = torch.optim.Adam(warm_optimizer_specs)\n",
    "\n",
    "joint_optimizer_lrs = {'features': 1e-4,\n",
    "                       'add_on_layers': 3e-3,\n",
    "                       'prototype_vectors': 3e-3}\n",
    "joint_lr_step_size = 5\n",
    "\n",
    "joint_optimizer_specs = [{\n",
    "    'params': ppnet.features.parameters(), \n",
    "    'lr': joint_optimizer_lrs['features'], \n",
    "    'weight_decay': 1e-3\n",
    "    }, \n",
    "    {\n",
    "    'params': ppnet.add_on_layers.parameters(), \n",
    "    'lr': joint_optimizer_lrs['add_on_layers'], \n",
    "    'weight_decay': 1e-3 },\n",
    "    {\n",
    "    'params': ppnet.prototype_vectors,\n",
    "    'lr': joint_optimizer_lrs['prototype_vectors']\n",
    "    }]\n",
    "\n",
    "joint_optimizer = torch.optim.Adam(joint_optimizer_specs)\n",
    "\n",
    "joint_lr_scheduler = torch.optim.lr_scheduler.StepLR(joint_optimizer, step_size=joint_lr_step_size, gamma=0.1)\n",
    "\n",
    "h_last_layer_optimizer_lr = 1e-4\n",
    "\n",
    "h_last_layer_optimizer_specs = [{\n",
    "    'params': ppnet.h_last_layer.parameters(), \n",
    "    'lr': h_last_layer_optimizer_lr\n",
    "    }]\n",
    "\n",
    "h_last_layer_optimizer = torch.optim.Adam(h_last_layer_optimizer_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224a04b-b253-4223-b6a0-944c9236d121",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def _train_or_test(model, dataloader, optimizer, class_specific, coefs, log):\n",
    "    is_train = optimizer is not None\n",
    "    start = time.time()\n",
    "    n_examples = 0\n",
    "    n_correct = 0\n",
    "    n_batches = 0\n",
    "    total_cross_entropy = 0\n",
    "    total_cluster_cost = 0\n",
    "    total_separation_cost = 0\n",
    "    total_avg_separation_cost = 0\n",
    "\n",
    "    for i, (image, label) in enumerate(dataloader):\n",
    "        input_imag = image \n",
    "        target_class = label\n",
    "        log('INFO: training with the images batch#{0}'.format(i))\n",
    "        grad_req = torch.enable_grad() if is_train else torch.no_grad()\n",
    "        with grad_req:\n",
    "            output, min_distances = model(input_imag)\n",
    "            # compute loss\n",
    "            cross_entropy = torch.nn.functional.cross_entropy(output, target_class)\n",
    "\n",
    "            max_dist = (ppnet.prototype_shape[1] * ppnet.prototype_shape[2] * ppnet.prototype_shape[3])\n",
    "\n",
    "            correct_class_indicators = torch.t(ppnet.prototype_class_identity[:, target_class]) # batch_size * num_prototypes\n",
    "            inverted_min_distances2correct_class, _ = torch.max((max_dist - min_distances) * correct_class_indicators, dim=1)  # max over all correct prototypes\n",
    "            cluster_cost = torch.mean(max_dist - inverted_min_distances2correct_class)\n",
    "\n",
    "            # calculate separation cost\n",
    "            wrong_class_indicators = 1 - correct_class_indicators\n",
    "            inverted_min_distances2wrong_class, _ = torch.max((max_dist - min_distances) * wrong_class_indicators, dim=1)\n",
    "            separation_cost = torch.mean(max_dist - inverted_min_distances2wrong_class)\n",
    "\n",
    "            # calculate avg cluster cost\n",
    "            avg_separation_cost = torch.sum(min_distances * wrong_class_indicators, dim=1) / torch.sum(wrong_class_indicators, dim=1)\n",
    "            avg_separation_cost = torch.mean(avg_separation_cost)\n",
    "                \n",
    "            l1_mask = 1 - torch.t(ppnet.prototype_class_identity) \n",
    "            l1 = (ppnet.h_last_layer.weight * l1_mask).norm(p=1)\n",
    "\n",
    "            # evaluation statistics\n",
    "            _, predicted = torch.max(output.data, dim=1)\n",
    "            n_examples += target_class.size(0)\n",
    "            n_correct += (predicted == target_class).sum().item()\n",
    "\n",
    "            n_batches += 1\n",
    "            total_cross_entropy += cross_entropy.item()\n",
    "            total_cluster_cost += cluster_cost.item()\n",
    "            total_separation_cost += separation_cost.item()\n",
    "            total_avg_separation_cost += avg_separation_cost.item()\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        if is_train:\n",
    "            loss = (coefs['crs_ent'] * cross_entropy + coefs['clst'] * cluster_cost + coefs['sep'] * separation_cost + coefs['l1'] * l1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        del input_imag\n",
    "        del target_class\n",
    "        del output\n",
    "        del predicted\n",
    "        del min_distances\n",
    "\n",
    "    end = time.time()\n",
    "    training_or_testing = 'training' if is_train else 'testing'\n",
    "    log('INFO: {} time: \\t{}'.format(training_or_testing, end -  start))\n",
    "    log('INFO: average cross entropy per batch: \\t{0}'.format(total_cross_entropy / n_batches))\n",
    "    log('INFO: average cluster loss per batch: \\t{0}'.format(total_cluster_cost / n_batches))\n",
    "    log('INFO: separation loss:\\t{0}'.format(total_separation_cost / n_batches))\n",
    "    log('INFO: avged separation loss:\\t{0}'.format(total_avg_separation_cost / n_batches))\n",
    "    log('INFO: accu: \\t{0}%'.format(n_correct / n_examples * 100))\n",
    "    \n",
    "    return n_correct / n_examples\n",
    "\n",
    "\n",
    "def tnt_train(model, dataloader, optimizer, class_specific, coefs, log):\n",
    "    assert(optimizer is not None)    \n",
    "    model.train()\n",
    "    return _train_or_test(model=model, dataloader=dataloader, optimizer=optimizer,\n",
    "                          class_specific=class_specific, coefs=coefs, log=log)\n",
    "\n",
    "\n",
    "def tnt_test(model, dataloader, class_specific, log):\n",
    "    model.eval()\n",
    "    return _train_or_test(model=model, dataloader=dataloader, optimizer=None,\n",
    "                          class_specific=class_specific, coefs=None, log=log)\n",
    "\n",
    "\n",
    "def tnt_last_only(model, log=print):\n",
    "    for p in ppnet.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in ppnet.add_on_layers.parameters():\n",
    "        p.requires_grad = False\n",
    "    ppnet.prototype_vectors.requires_grad = False\n",
    "    for p in ppnet.h_last_layer.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def tnt_warm_only(model, log=print):\n",
    "    for p in ppnet.features.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in ppnet.add_on_layers.parameters():\n",
    "        p.requires_grad = True\n",
    "    ppnet.prototype_vectors.requires_grad = True\n",
    "    for p in ppnet.h_last_layer.parameters():\n",
    "        p.requires_grad = True\n",
    "    \n",
    "\n",
    "def tnt_joint(model, log=print):\n",
    "    for p in ppnet.features.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in ppnet.add_on_layers.parameters():\n",
    "        p.requires_grad = True\n",
    "    ppnet.prototype_vectors.requires_grad = True\n",
    "    for p in ppnet.h_last_layer.parameters():\n",
    "        p.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f40521-f6c6-41e3-b357-b8b029d771a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rf_prototype(img_size, prototype_patch_index, protoL_rf_info):\n",
    "    img_index = prototype_patch_index[0]\n",
    "    height_index = prototype_patch_index[1]\n",
    "    width_index = prototype_patch_index[2]\n",
    "    rf_indices = compute_rf_protoL_at_spatial_location(img_size,\n",
    "                                                       height_index,\n",
    "                                                       width_index,\n",
    "                                                       protoL_rf_info)\n",
    "    return [img_index, rf_indices[0], rf_indices[1],\n",
    "            rf_indices[2], rf_indices[3]]\n",
    "\n",
    "def compute_rf_protoL_at_spatial_location(img_size, height_index, width_index, protoL_rf_info):\n",
    "    n = protoL_rf_info[0]\n",
    "    j = protoL_rf_info[1]\n",
    "    r = protoL_rf_info[2]\n",
    "    start = protoL_rf_info[3]\n",
    "    assert(height_index < n)\n",
    "    assert(width_index < n)\n",
    "\n",
    "    center_h = start + (height_index*j)\n",
    "    center_w = start + (width_index*j)\n",
    "\n",
    "    rf_start_height_index = max(int(center_h - (r/2)), 0)\n",
    "    rf_end_height_index = min(int(center_h + (r/2)), img_size)\n",
    "\n",
    "    rf_start_width_index = max(int(center_w - (r/2)), 0)\n",
    "    rf_end_width_index = min(int(center_w + (r/2)), img_size)\n",
    "\n",
    "    return [rf_start_height_index, rf_end_height_index,\n",
    "            rf_start_width_index, rf_end_width_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb2522-2067-4e5a-982c-bf7f018aeb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# from receptive_field import compute_rf_prototype\n",
    "# from helpers import makedir, find_high_activation_crop\n",
    "\n",
    "def preprocess(x, mean, std):\n",
    "    assert x.size(1) == 3\n",
    "    y = torch.zeros_like(x)\n",
    "    for i in range(3):\n",
    "        y[:, i, :, :] = (x[:, i, :, :] - mean[i]) / std[i]\n",
    "    return y\n",
    "\n",
    "\n",
    "def preprocess_input_function(x):\n",
    "    '''\n",
    "    allocate new tensor like x and apply the normalization used in the\n",
    "    pretrained model\n",
    "    '''\n",
    "    return preprocess(x, mean=preprocess_mean, std=preprocess_std)\n",
    "\n",
    "def find_high_activation_crop(activation_map, percentile=95):\n",
    "    threshold = np.percentile(activation_map, percentile)\n",
    "    mask = np.ones(activation_map.shape)\n",
    "    mask[activation_map < threshold] = 0\n",
    "    lower_y, upper_y, lower_x, upper_x = 0, 0, 0, 0\n",
    "    for i in range(mask.shape[0]):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            lower_y = i\n",
    "            break\n",
    "    for i in reversed(range(mask.shape[0])):\n",
    "        if np.amax(mask[i]) > 0.5:\n",
    "            upper_y = i\n",
    "            break\n",
    "    for j in range(mask.shape[1]):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            lower_x = j\n",
    "            break\n",
    "    for j in reversed(range(mask.shape[1])):\n",
    "        if np.amax(mask[:,j]) > 0.5:\n",
    "            upper_x = j\n",
    "            break\n",
    "    return lower_y, upper_y+1, lower_x, upper_x+1\n",
    "\n",
    "# push each prototype to the nearest patch in the training set\n",
    "def push_prototypes(dataloader, # pytorch dataloader (must be unnormalized in [0,1])\n",
    "                    prototype_network, # pytorch network with prototype_vectors\n",
    "                    class_specific=True,\n",
    "                    preprocess_input_function=None, # normalize if needed\n",
    "                    prototype_layer_stride=1,\n",
    "                    root_dir_for_saving_prototypes=None, # if not None, prototypes will be saved here\n",
    "                    epoch_number=None, # if not provided, prototypes saved previously will be overwritten\n",
    "                    prototype_img_filename_prefix=None,\n",
    "                    prototype_self_act_filename_prefix=None,\n",
    "                    proto_bound_boxes_filename_prefix=None,\n",
    "                    save_prototype_class_identity=True, # which class the prototype image comes from\n",
    "                    log=print,\n",
    "                    prototype_activation_function_in_numpy=None):\n",
    "\n",
    "    prototype_network.eval()\n",
    "\n",
    "    prototype_shape = prototype_network.prototype_shape\n",
    "    n_prototypes = prototype_network.num_prototypes\n",
    "    # saves the closest distance seen so far\n",
    "    global_min_proto_dist = np.full(n_prototypes, np.inf)\n",
    "    # saves the patch representation that gives the current smallest distance\n",
    "    global_min_fmap_patches = np.zeros(\n",
    "        [n_prototypes,\n",
    "         prototype_shape[1],\n",
    "         prototype_shape[2],\n",
    "         prototype_shape[3]])\n",
    "\n",
    "    '''\n",
    "    proto_rf_boxes and proto_bound_boxes column:\n",
    "    0: image index in the entire dataset\n",
    "    1: height start index\n",
    "    2: height end index\n",
    "    3: width start index\n",
    "    4: width end index\n",
    "    5: (optional) class identity\n",
    "    '''\n",
    "\n",
    "    proto_rf_boxes = np.full(shape=[n_prototypes, 6], fill_value=-1)\n",
    "    proto_bound_boxes = np.full(shape=[n_prototypes, 6], fill_value=-1)\n",
    "\n",
    "    proto_epoch_dir = os.path.join(root_dir_for_saving_prototypes, 'epoch-'+str(epoch_number))\n",
    "    makedir(proto_epoch_dir)\n",
    "\n",
    "    search_batch_size = dataloader.batch_size\n",
    "    num_classes = prototype_network.num_classes\n",
    "\n",
    "    for push_iter, (search_batch_input, search_y) in enumerate(dataloader):\n",
    "        start_index_of_search_batch = push_iter * search_batch_size\n",
    "        update_prototypes_on_batch(search_batch_input,\n",
    "                                   start_index_of_search_batch,\n",
    "                                   prototype_network,\n",
    "                                   global_min_proto_dist,\n",
    "                                   global_min_fmap_patches,\n",
    "                                   proto_rf_boxes,\n",
    "                                   proto_bound_boxes,\n",
    "                                   class_specific=class_specific,\n",
    "                                   search_y=search_y,\n",
    "                                   num_classes=num_classes,\n",
    "                                   preprocess_input_function=preprocess_input_function,\n",
    "                                   prototype_layer_stride=prototype_layer_stride,\n",
    "                                   dir_for_saving_prototypes=proto_epoch_dir,\n",
    "                                   prototype_img_filename_prefix=prototype_img_filename_prefix,\n",
    "                                   prototype_self_act_filename_prefix=prototype_self_act_filename_prefix,\n",
    "                                   prototype_activation_function_in_numpy=prototype_activation_function_in_numpy)\n",
    "\n",
    "    np.save(os.path.join(proto_epoch_dir, proto_bound_boxes_filename_prefix + '-receptive_field' + str(epoch_number) + '.npy'), proto_rf_boxes)\n",
    "    np.save(os.path.join(proto_epoch_dir, proto_bound_boxes_filename_prefix + str(epoch_number) + '.npy'), proto_bound_boxes)\n",
    "\n",
    "    prototype_update = np.reshape(global_min_fmap_patches, tuple(prototype_shape))\n",
    "    prototype_network.prototype_vectors.data.copy_(torch.tensor(prototype_update, dtype=torch.float32))\n",
    "\n",
    "# update each prototype for current search batch\n",
    "def update_prototypes_on_batch(search_batch_input,\n",
    "                               start_index_of_search_batch,\n",
    "                               prototype_network,\n",
    "                               global_min_proto_dist, # this will be updated\n",
    "                               global_min_fmap_patches, # this will be updated\n",
    "                               proto_rf_boxes, # this will be updated\n",
    "                               proto_bound_boxes, # this will be updated\n",
    "                               class_specific=True,\n",
    "                               search_y=None, # required if class_specific == True\n",
    "                               num_classes=None, # required if class_specific == True\n",
    "                               preprocess_input_function=None,\n",
    "                               prototype_layer_stride=1,\n",
    "                               dir_for_saving_prototypes=None,\n",
    "                               prototype_img_filename_prefix=None,\n",
    "                               prototype_self_act_filename_prefix=None,\n",
    "                               prototype_activation_function_in_numpy=None):\n",
    "\n",
    "    prototype_network.eval()\n",
    "    search_batch = preprocess_input_function(search_batch_input)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        protoL_input_tmp, proto_dist_tmp = prototype_network.push_forward(search_batch)\n",
    "\n",
    "    protoL_input_ = protoL_input_tmp.numpy()\n",
    "    proto_dist_ = proto_dist_tmp.numpy()\n",
    "    \n",
    "    class_to_img_index_dict = {key: [] for key in range(num_classes)}\n",
    "    # img_y is the image's integer label\n",
    "    for img_index, img_y in enumerate(search_y):\n",
    "        img_label = img_y.item()\n",
    "        class_to_img_index_dict[img_label].append(img_index)\n",
    "\n",
    "    prototype_shape = prototype_network.prototype_shape\n",
    "    n_prototypes = prototype_shape[0]\n",
    "    proto_h = prototype_shape[2]\n",
    "    proto_w = prototype_shape[3]\n",
    "    max_dist = prototype_shape[1] * prototype_shape[2] * prototype_shape[3]\n",
    "\n",
    "    for j in range(n_prototypes):\n",
    "        target_class = torch.argmax(prototype_network.prototype_class_identity[j]).item()\n",
    "        if len(class_to_img_index_dict[target_class]) == 0:\n",
    "            continue\n",
    "        proto_dist_j = proto_dist_[class_to_img_index_dict[target_class]][:,j,:,:]\n",
    "\n",
    "        batch_min_proto_dist_j = np.amin(proto_dist_j)\n",
    "        if batch_min_proto_dist_j < global_min_proto_dist[j]:\n",
    "            batch_argmin_proto_dist_j = list(np.unravel_index(np.argmin(proto_dist_j, axis=None), proto_dist_j.shape))\n",
    "\n",
    "            ''' change the argmin index from the index among images of the target class to the index in the entire search batch '''\n",
    "            batch_argmin_proto_dist_j[0] = class_to_img_index_dict[target_class][batch_argmin_proto_dist_j[0]]\n",
    "\n",
    "            # retrieve the corresponding feature map patch\n",
    "            img_index_in_batch = batch_argmin_proto_dist_j[0]\n",
    "            fmap_height_start_index = batch_argmin_proto_dist_j[1] * prototype_layer_stride\n",
    "            fmap_height_end_index = fmap_height_start_index + proto_h\n",
    "            fmap_width_start_index = batch_argmin_proto_dist_j[2] * prototype_layer_stride\n",
    "            fmap_width_end_index = fmap_width_start_index + proto_w\n",
    "\n",
    "            batch_min_fmap_patch_j = protoL_input_[img_index_in_batch, :,\n",
    "                                                   fmap_height_start_index:fmap_height_end_index,\n",
    "                                                   fmap_width_start_index:fmap_width_end_index]\n",
    "\n",
    "            global_min_proto_dist[j] = batch_min_proto_dist_j\n",
    "            global_min_fmap_patches[j] = batch_min_fmap_patch_j\n",
    "            \n",
    "            # get the receptive field boundary of the image patch that generates the representation\n",
    "            protoL_rf_info = prototype_network.proto_layer_rf_info\n",
    "            rf_prototype_j = compute_rf_prototype(search_batch.size(2), batch_argmin_proto_dist_j, protoL_rf_info)\n",
    "            \n",
    "            # get the whole image\n",
    "            original_img_j = search_batch_input[rf_prototype_j[0]]\n",
    "            original_img_j = original_img_j.numpy()\n",
    "            original_img_j = np.transpose(original_img_j, (1, 2, 0))\n",
    "            original_img_size = original_img_j.shape[0]\n",
    "            \n",
    "            # crop out the receptive field\n",
    "            rf_img_j = original_img_j[rf_prototype_j[1]:rf_prototype_j[2],\n",
    "                                      rf_prototype_j[3]:rf_prototype_j[4], :]\n",
    "            \n",
    "            # save the prototype receptive field information\n",
    "            proto_rf_boxes[j, 0] = rf_prototype_j[0] + start_index_of_search_batch\n",
    "            proto_rf_boxes[j, 1] = rf_prototype_j[1]\n",
    "            proto_rf_boxes[j, 2] = rf_prototype_j[2]\n",
    "            proto_rf_boxes[j, 3] = rf_prototype_j[3]\n",
    "            proto_rf_boxes[j, 4] = rf_prototype_j[4]\n",
    "            if proto_rf_boxes.shape[1] == 6 and search_y is not None:\n",
    "                proto_rf_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
    "\n",
    "            # find the highly activated region of the original image\n",
    "            proto_dist_img_j = proto_dist_[img_index_in_batch, j, :, :]\n",
    "            if prototype_network.prototype_activation_function == 'log':\n",
    "                proto_act_img_j = np.log((proto_dist_img_j + 1) / (proto_dist_img_j + prototype_network.epsilon))\n",
    "            elif prototype_network.prototype_activation_function == 'linear':\n",
    "                proto_act_img_j = max_dist - proto_dist_img_j\n",
    "            else:\n",
    "                proto_act_img_j = prototype_activation_function_in_numpy(proto_dist_img_j)\n",
    "            upsampled_act_img_j = cv2.resize(proto_act_img_j, dsize=(original_img_size, original_img_size),\n",
    "                                             interpolation=cv2.INTER_CUBIC)\n",
    "            proto_bound_j = find_high_activation_crop(upsampled_act_img_j)\n",
    "            # crop out the image patch with high activation as prototype image\n",
    "            proto_img_j = original_img_j[proto_bound_j[0]:proto_bound_j[1],\n",
    "                                         proto_bound_j[2]:proto_bound_j[3], :]\n",
    "\n",
    "            # save the prototype boundary (rectangular boundary of highly activated region)\n",
    "            proto_bound_boxes[j, 0] = proto_rf_boxes[j, 0]\n",
    "            proto_bound_boxes[j, 1] = proto_bound_j[0]\n",
    "            proto_bound_boxes[j, 2] = proto_bound_j[1]\n",
    "            proto_bound_boxes[j, 3] = proto_bound_j[2]\n",
    "            proto_bound_boxes[j, 4] = proto_bound_j[3]\n",
    "            if proto_bound_boxes.shape[1] == 6 and search_y is not None:\n",
    "                proto_bound_boxes[j, 5] = search_y[rf_prototype_j[0]].item()\n",
    "\n",
    "    del class_to_img_index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac8e6a-9bfc-4b90-b1df-dc003711f0ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "log('INFO: start training the model ......')\n",
    "num_train_epochs = 1000\n",
    "num_warm_epochs = 5\n",
    "\n",
    "push_start = 10\n",
    "push_epochs = [i for i in range(num_train_epochs) if i % 10 == 0]\n",
    "\n",
    "coefs = {'crs_ent': 1, 'clst': 0.8, 'sep': -0.08, 'l1': 1e-4}\n",
    "class_specific = True\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    log('INFO:epoch: \\t{0}'.format(epoch))\n",
    "\n",
    "    if epoch < num_warm_epochs:\n",
    "        tnt_warm_only(model=ppnet, log=log)\n",
    "        _ = tnt_train(model=ppnet, dataloader=train_loader, optimizer=warm_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "    else:\n",
    "        tnt_joint(model=ppnet, log=log)\n",
    "        joint_lr_scheduler.step()\n",
    "        _ = tnt_train(model=ppnet, dataloader=train_loader, optimizer=joint_optimizer,\n",
    "                      class_specific=class_specific, coefs=coefs, log=log)\n",
    "\n",
    "    accu = tnt_test(model=ppnet, dataloader=test_loader,\n",
    "                    class_specific=class_specific, log=log)\n",
    "\n",
    "    if epoch >= push_start and epoch in push_epochs:\n",
    "        push_prototypes(\n",
    "            train_push_loader, # pytorch dataloader (must be unnormalized in [0,1])\n",
    "            prototype_network=ppnet, # pytorch network with prototype_vectors\n",
    "            class_specific=class_specific,\n",
    "            preprocess_input_function=preprocess_input_function, # normalize if needed\n",
    "            prototype_layer_stride=1,\n",
    "            root_dir_for_saving_prototypes=img_dir, # if not None, prototypes will be saved here\n",
    "            epoch_number=epoch, # if not provided, prototypes saved previously will be overwritten\n",
    "            prototype_img_filename_prefix=prototype_img_filename_prefix,\n",
    "            prototype_self_act_filename_prefix=prototype_self_act_filename_prefix,\n",
    "            proto_bound_boxes_filename_prefix=proto_bound_boxes_filename_prefix,\n",
    "            save_prototype_class_identity=True,\n",
    "            log=log)\n",
    "        accu = tnt_test(model=ppnet, dataloader=test_loader,\n",
    "                        class_specific=class_specific, log=log)\n",
    "\n",
    "        if prototype_activation_function != 'linear':\n",
    "            tnt_last_only(model=ppnet, log=log)\n",
    "            for i in range(20):\n",
    "                _ = tnt_train(model=ppnet, dataloader=train_loader, optimizer=h_last_layer_optimizer,\n",
    "                              class_specific=class_specific, coefs=coefs, log=log)\n",
    "                accu = tnt_test(model=ppnet, dataloader=test_loader,\n",
    "                                class_specific=class_specific, log=log)\n",
    "\n",
    "logclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f187b8-ae2e-4d37-92c6-9f77e28050b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
